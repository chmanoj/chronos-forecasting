training_data_paths:
- "./sample_training_data.arrow"
probability:
- 1.0
context_length: 64
prediction_length: 24
min_past: 24
max_steps: 100  # Very few steps for quick training
save_steps: 50
log_steps: 10
per_device_train_batch_size: 4
learning_rate: 0.001
optim: adamw_torch_fused
num_samples: 10
shuffle_buffer_length: 100
gradient_accumulation_steps: 1
model_id: google/t5-efficient-tiny
model_type: seq2seq
random_init: true
tie_embeddings: true
output_dir: ./output/
tf32: false  # Disable for compatibility
torch_compile: false  # Disable for compatibility
tokenizer_class: "MeanScaleUniformBins"
tokenizer_kwargs:
  low_limit: -10.0
  high_limit: 10.0
n_tokens: 1024  # Smaller vocab for faster training
lr_scheduler_type: linear
warmup_ratio: 0.0
dataloader_num_workers: 0  # Disable multiprocessing for simplicity
max_missing_prop: 0.9
use_eos_token: true
seed: 42

# MOE-specific parameters
use_moe: true
num_experts: 4
num_active_experts: 2
load_balancing_weight: 0.01
router_hidden_dim: 128